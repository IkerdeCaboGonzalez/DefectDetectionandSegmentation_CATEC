{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iUwIDPbA1NAc"
      },
      "outputs": [],
      "source": [
        "# Importamos las librerías\n",
        "\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tifffile\n",
        "import sys\n",
        "import torch.nn as nn\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m pip install tensorflow\n",
        "!python -m pip install keras\n",
        "!python -m pip install numpy\n",
        "!python -m pip install Pillow\n",
        "!python -m pip install os\n",
        "!python -m pip install scipy\n",
        "!python -m pip install scikit-learn\n",
        "!python -m pip install matplotlib.pyplot\n",
        "!python -m pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9Ny-bAoFG_f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(1.0)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ruta_no_marcadas=r'C:\\Users\\unidad-epa\\Desktop\\Iker\\Scripts\\Reconstrucción defectos\\Defecto_1'\n",
        "ruta_marcados=r'C:\\Users\\unidad-epa\\Desktop\\Iker\\Scripts\\Reconstrucción defectos\\Defecto_1_marcado'\n",
        "imagenes_tif_marcado=[]\n",
        "imagenes_tif=[]\n",
        "\n",
        "def cargar_imagenes(ruta):\n",
        "  contenedor=[]\n",
        "  for archivo in os.listdir(ruta):\n",
        "    ruta_completa=os.path.join(ruta, archivo)\n",
        "    if os.path.isfile(ruta_completa):\n",
        "      imagen=Image.open(ruta_completa)\n",
        "      contenedor.append(imagen)\n",
        "    return contenedor\n",
        "  \n",
        "\n",
        "archivos_tif = np.array(cargar_imagenes(ruta_no_marcadas))/255\n",
        "archivos_tif.max()\n",
        "#imagenes_tif_marcado = np.array(cargar_imagenes(ruta_marcados, imagenes_tif_marcado))/255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([], dtype=float64)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "imagenes_tif_marcado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUjYdvyUJDR3",
        "outputId": "a6864032-ea32-4630-b775-4a27e6426cd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ImageWidth: 991\n",
            "ImageLength: 1022\n",
            "BitsPerSample: (8, 8, 8)\n",
            "Compression: 32773\n",
            "PhotometricInterpretation: 2\n",
            "StripOffsets: (8, 440, 872, 1304, 1736, 2168, 2600, 3032, 3464, 3896, 4328, 4760, 5192, 5624, 6056, 6488, 6920, 7352, 7784, 8216, 8648, 9080, 9512, 9944, 10376, 10812, 12560, 15906, 20228, 25164, 30548, 36422, 42794, 49444, 56476, 63610, 71064, 78894, 86930, 95314, 103940, 112738, 121712, 130808, 140146, 149574, 159140, 168786, 178580, 188520, 198478, 208480, 218564, 228708, 238878, 249100, 259302, 269550, 279820, 290050, 300278, 310412, 320508, 330528, 340546, 350446, 360256, 369970, 379598, 389108, 398480, 407782, 416848, 425770, 434492, 443038, 451366, 459490, 467320, 474884, 482190, 489108, 495690, 501946, 507636, 512860, 517538, 521472, 524368, 525548, 525980, 526412, 526844, 527276, 527708, 528140, 528572, 529004, 529436, 529868, 530300, 530732, 531164, 531596, 532028, 532460, 532892, 533324, 533756, 534188, 534620, 535124, 535625, 536411)\n",
            "SampleFormat: (1, 1, 1)\n",
            "SamplesPerPixel: 3\n",
            "RowsPerStrip: 9\n",
            "StripByteCounts: (432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 436, 1748, 3346, 4322, 4936, 5384, 5874, 6372, 6650, 7032, 7134, 7454, 7830, 8036, 8384, 8626, 8798, 8974, 9096, 9338, 9428, 9566, 9646, 9794, 9940, 9958, 10002, 10084, 10144, 10170, 10222, 10202, 10248, 10270, 10230, 10228, 10134, 10096, 10020, 10018, 9900, 9810, 9714, 9628, 9510, 9372, 9302, 9066, 8922, 8722, 8546, 8328, 8124, 7830, 7564, 7306, 6918, 6582, 6256, 5690, 5224, 4678, 3934, 2896, 1180, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 432, 504, 501, 786, 423)\n",
            "XResolution: 96.0\n",
            "YResolution: 96.0\n",
            "PlanarConfiguration: 1\n",
            "32997: 1\n",
            "ResolutionUnit: 2\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "from PIL.TiffTags import TAGS\n",
        "\n",
        "# Cargar imagen\n",
        "imagen = Image.open('/content/Data/Defectos/ProbetaAE_008.tif')\n",
        "\n",
        "# Obtener metadatos\n",
        "metadatos = imagen.tag_v2\n",
        "for tag_id, valor in metadatos.items():\n",
        "    nombre_tag = TAGS.get(tag_id, tag_id)\n",
        "    print(f\"{nombre_tag}: {valor}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHM2Kz5QZ0xE"
      },
      "source": [
        "Vamos a probar a hacer la reconstrucción con una red neuronal separada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2Ke9ruYmW5Z"
      },
      "outputs": [],
      "source": [
        "def crop_to_match(encoder_tensor, decoder_tensor):\n",
        "    d, h, w = decoder_tensor.shape[2:]\n",
        "    return encoder_tensor[:, :, :d, :h, :w]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1G-d4HwZ7SH",
        "outputId": "c6a02510-9809-4111-c527-1c2a251bebd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of upsampled_b before concat: torch.Size([1, 64, 5, 510, 494])\n",
            "Shape of cropped_e2 before concat: torch.Size([1, 64, 4, 510, 494])\n",
            "Shape of upsampled_d2 before concat: torch.Size([1, 32, 9, 1020, 988])\n",
            "Shape of cropped_e1 before concat: torch.Size([1, 32, 8, 1020, 988])\n",
            "torch.Size([1, 1, 8, 1020, 988])\n"
          ]
        }
      ],
      "source": [
        "# Cargamos las imágenes clasificadas como defectuosas\n",
        "\n",
        "defectos = np.stack(imagenes_tif, axis=0)\n",
        "\n",
        "# Definimos el modelo\n",
        "\n",
        "class UNet3D(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1): # Constructor de la clase\n",
        "      super().__init__()\n",
        "      def CBR(in_c, out_c):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv3d(in_c, out_c, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(out_c, out_c, 3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "      self.enc1 = CBR(in_channels, 32)\n",
        "      self.pool1 = nn.MaxPool3d(2)\n",
        "      self.enc2 = CBR(32, 64)\n",
        "      self.pool2 = nn.MaxPool3d(2)\n",
        "\n",
        "      self.bottleneck = CBR(64, 128)\n",
        "\n",
        "      # Añadimos output_padding=(1, 0, 0) para corregir un posible desfase de 1 voxel en la dimensión de profundidad\n",
        "      # entre el upsampling y el tensor del encoder correspondiente.\n",
        "      self.up2 = nn.ConvTranspose3d(128, 64, 2, stride=2, output_padding=(1, 0, 0))\n",
        "      self.dec2 = CBR(128, 64)\n",
        "      self.up1 = nn.ConvTranspose3d(64, 32, 2, stride=2, output_padding=(1, 0, 0))\n",
        "      self.dec1 = CBR(64, 32)\n",
        "      self.out = nn.Conv3d(32, out_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.pool1(e1))\n",
        "        b = self.bottleneck(self.pool2(e2))\n",
        "\n",
        "        # Upsample the bottleneck features\n",
        "        upsampled_b = self.up2(b)\n",
        "\n",
        "     # Determine the target spatial shape by taking the minimum size\n",
        "        # along each spatial dimension (D, H, W)\n",
        "        target_depth = min(upsampled_b.shape[2], e2.shape[2])\n",
        "        target_height = min(upsampled_b.shape[3], e2.shape[3])\n",
        "        target_width = min(upsampled_b.shape[4], e2.shape[4])\n",
        "\n",
        "        # Crop both tensors to the target spatial dimensions\n",
        "        cropped_upsampled_b = upsampled_b[:, :, :target_depth, :target_height, :target_width]\n",
        "\n",
        "        cropped_e2 = crop_to_match(e2, upsampled_b)\n",
        "\n",
        "        # ### Add print statements here to check shapes BEFORE concatenation ###\n",
        "        print(\"Shape of upsampled_b before concat:\", upsampled_b.shape)\n",
        "        print(\"Shape of cropped_e2 before concat:\", cropped_e2.shape)\n",
        "        # ######################################################################\n",
        "\n",
        "\n",
        "        # Concatenate along the channel dimension (dim=1)\n",
        "        d2 = self.dec2(torch.cat([cropped_upsampled_b, cropped_e2], dim=1))\n",
        "\n",
        "        # Repeat the process for the next level\n",
        "        upsampled_d2 = self.up1(d2)\n",
        "\n",
        "        # Determine the target spatial shape from the upsampled tensor for this level\n",
        "        target_depth_up1, target_height_up1, target_width_up1 = upsampled_d2.shape[2:]\n",
        "\n",
        "        # Crop the corresponding encoder tensor (e1) to match the target dimensions\n",
        "\n",
        "        cropped_e1 = crop_to_match(e1, upsampled_d2)\n",
        "        cropped_upsampled_d2 = crop_to_match(upsampled_d2, e1)\n",
        "\n",
        "        # ### Add print statements here to check shapes BEFORE concatenation ###\n",
        "        print(\"Shape of upsampled_d2 before concat:\", upsampled_d2.shape)\n",
        "        print(\"Shape of cropped_e1 before concat:\", cropped_e1.shape)\n",
        "        # ######################################################################\n",
        "\n",
        "        d1 = self.dec1(torch.cat([cropped_upsampled_d2, cropped_e1], dim=1))\n",
        "\n",
        "        return torch.sigmoid(self.out(d1))\n",
        "\n",
        "# Inferencia (keep the rest of the inference code as is)\n",
        "model = UNet3D(in_channels=1, out_channels=1)  # Instanciar la red\n",
        "model.eval()\n",
        "\n",
        "imagenes_gris = [img.convert('L') for img in imagenes_tif]\n",
        "array_volumen=np.stack([np.array(img) for img in imagenes_gris])\n",
        "volume_tensor = torch.tensor(array_volumen, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred_mask = model(volume_tensor)  # Output shape: [1, 1, D, H, W]\n",
        "    pred_mask_bin = (pred_mask > 0.5).float()\n",
        "\n",
        "print(pred_mask.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UGH3Bw4gXDA",
        "outputId": "deb1a23d-81b9-4594-be1c-fff9d15ff8c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of upsampled_b before concat: torch.Size([1, 64, 5, 510, 494])\n",
            "Shape of cropped_e2 before concat: torch.Size([1, 64, 4, 510, 494])\n",
            "Shape of upsampled_d2 before concat: torch.Size([1, 32, 9, 1020, 988])\n",
            "Shape of cropped_e1 before concat: torch.Size([1, 32, 8, 1020, 988])\n"
          ]
        }
      ],
      "source": [
        "# Inferencia\n",
        "\n",
        "model = UNet3D(in_channels=1, out_channels=1)  # Instanciar la red\n",
        "model.eval()\n",
        "\n",
        "# Vamos a pasar las imágenes a escalas de grises (se supone en general mejor para el modelo, si es relevante el color se puedes dejar los tres canales)\n",
        "imagenes_gris = [img.convert('L') for img in imagenes_tif]\n",
        "array_volumen=np.stack([np.array(img) for img in imagenes_gris])\n",
        "volume_tensor = torch.tensor(array_volumen, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred_mask = model(volume_tensor)  # Output shape: [1, 1, D, H, W]\n",
        "    pred_mask_bin = (pred_mask > 0.5).float()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tbij9xySq9Ko"
      },
      "outputs": [],
      "source": [
        "pred_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wfg6_VgJgkE6",
        "outputId": "d7c26877-d585-4707-a3a1-10e99ad9eadd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Volumen estimado: 0.00 mm³\n"
          ]
        }
      ],
      "source": [
        "z_spacing_mm = 25.4/96\n",
        "\n",
        "voxel_volume_mm3 = (1 / 96)**2 * z_spacing_mm\n",
        "volume_mm3 = torch.sum(pred_mask_bin).item() * voxel_volume_mm3\n",
        "print(f\"Volumen estimado: {volume_mm3:.2f} mm³\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je4XL8E8rSC_"
      },
      "source": [
        "El entrenamiento de una red de este tipo requiere que los datos de entrenamiento estén etiquetados pixel a pixel, es decir, que se le indique cada pixel correspondiente a un defecto. Va a estar complicado."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
